{
    "name": "crawler",
    "version": "2.0.0",
    "description": "Crawler is a ready-to-use web spider that works with proxies, asynchrony, rate limit, configurable request pools, jQuery, and seamless HTTP/2 support.",
    "repository": {
        "type": "git",
        "url": "https://github.com/bda-research/node-crawler.git"
    },
    "exports": "./dist/index.js",
    "scripts": {
        "build": "tsc",
        "test": "ava"
    },
    "engines": {
        "node": ">=18"
    },
    "type": "module",
    "keywords": [
        "javascript",
        "crawler",
        "spider",
        "scraper",
        "scraping",
        "jquery",
        "nodejs",
        "http",
        "https",
        "http2",
        "got",
        "request",
        "url",
        "network",
        "gzip"
    ],
    "license": "MIT",
    "dependencies": {
        "cheerio": "1.0.0-rc.12",
        "got": "^14.4.1",
        "hpagent": "^1.2.0",
        "http2-wrapper": "^2.2.1",
        "iconv-lite": "^0.6.3",
        "seenreq": "^3.0.0",
        "tslog": "^4.9.3"
    },
    "devDependencies": {
        "@eslint/js": "^9.5.0",
        "@types/got": "^9.6.12",
        "@types/node": "^20.14.5",
        "ava": "^6.1.3",
        "chai": "^5.1.1",
        "eslint": "~9.4.0",
        "globals": "^15.6.0",
        "nock": "^13.5.4",
        "tough-cookie": "^4.1.4",
        "tsx": "^4.15.6",
        "typescript": "^5.4.5",
        "typescript-eslint": "8.0.0-alpha.27"
    },
    "ava": {
        "files": [
            "test/*.js",
            "!test/*test.js"
        ],
        "timeout": "20s",
        "extensions": {
            "js": true
        },
        "failFast": true,
        "verbose": true
    }
}